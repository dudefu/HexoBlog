---
title: java.sql.SQLException: No suitable driver 解决方案
date: 2019-05-09 10:30:14
tags: Spark
categories: 大数据
---
报错代码：
```Java
Exception in thread "main" java.sql.SQLException: No suitable driver	at java.sql.DriverManager.getDriver(DriverManager.java:315)	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:85)	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$7.apply(JDBCOptions.scala:85)	at scala.Option.getOrElse(Option.scala:121)	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:84)	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:35)	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:60)	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
<!-- more -->	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:501)	at com.xinyi.multiCollision.service.JobService$.dealWithMoreData(JobService.scala:406)	at com.xinyi.multiCollision.sparkJob.MultiCollisionApp$.main(MultiCollisionApp.scala:70)	at com.xinyi.multiCollision.sparkJob.MultiCollisionApp.main(MultiCollisionApp.scala)	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.lang.reflect.Method.invoke(Method.java:498)	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
```
一般情况下，此报错原因是因为缺少连接数据库的驱动。
正常情况下，连接参数，驱动都是能考虑到的。但是在连接参数，驱动已添加的情况下也会报错。本人spark项目在IDEA正常运行，但是spark-submit的时候出现此错误（之前是可以正常运行的，后面报错，怀疑跟集群环境有关）。提交脚本已经配置了参数--conf spark.yarn.jars （虽然已经配置，驱动，jar包有可能不加载进去）。此时的解决方案：
将